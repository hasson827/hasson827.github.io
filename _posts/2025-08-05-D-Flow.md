---
title: "D-Flow Differentiating through Flows for Controlled Generation"
mathjax: true
layout: post
categories: Generative Modeling
---

# D-Flow: Differentiating through Flows for Controlled Generation

---

## 引言

控制生成是指从生成模型中获得特定结果的能力，应用于条件生成、逆问题和样本编辑等场景。本文聚焦于扩散/流匹配(Flow-Matching)生成模型的控制生成，因为这些是当前最先进的生成方法。

### 现有方法分类

论文将控制生成方法分为三大类：

- 条件训练(Conditional Training)：在训练过程中将条件作为额外输入，代表工作包括Song et al. (2020)、Dhariwal & Nichol (2021)等。优点是性能优异，缺点是需要针对特定任务训练模型，成本高昂。

- 无需训练的方法(Training-Free Approaches)：修改预训练模型的生成过程，添加额外引导，代表工作包括Bar-Tal et al. (2023)、Yu et al. (2023)等。优点是无需重新训练模型，缺点是通常基于对生成过程的强假设，主要限于对目标线性观测的情况。

- 变分视角(Variational Perspective)：将控制生成视为优化问题，代表工作包括Graikos et al. (2023)、Mardani et al. (2023)等。只需要可微分的成本函数来强制执行控制，D-Flow属于这一类别。

### D-Flow框架核心思想

D-Flow通过微分生成过程的ODE采样过程，优化源(噪声)点$x_0$。关键观察是：对于使用高斯概率路径训练的扩散/流匹配模型，通过生成过程对源点进行微分可以将梯度$\nabla_x L$投影到"数据流形"上，隐式地将有价值的数据先验注入到优化过程中。

核心优化问题为：
$$
\min_{x_0} L(x)
$$
其中$x$是通过ODE从$x_0$生成的样本，$L$是表示所需控制的任意成本函数。

本文的两点创新：
1. 考虑使用高斯概率路径训练的一般流模型(包括扩散和流匹配模型)
2. 证明并展示通过流的微分的归纳偏置适用于更广泛的问题

## 前置知识：流模型基础

流模型包括连续归一化流(CNFs)和确定性采样的扩散模型。生成过程如下：

首先从源(噪声)分布采样：$x(0) \sim p_0(x_0)$，然后解常微分方程(ODE)：
$$
x'(t) = u_t(x(t))
$$
从时间$t=0$到$t=1$，使用预定的速度场$u:[0,1]\times\mathbb{R}^d\rightarrow\mathbb{R}^d$。文中约定$t=0$对应噪声，$t=1$对应数据。

## 通过源点优化控制生成

D-Flow框架的核心思想是将控制生成问题表述为**源点优化问题**。给定一个预训练(冻结)的流模型$u_t(x)$（由神经网络表示）和某个成本函数$L: \mathbb{R}^d \rightarrow \mathbb{R}^+$，目标是找到在流模型分布$p_1$下可能的样本$x$，使得成本$L(x)$较低。

### 优化问题表述

论文提出以下优化问题：
$$
\min_{x_0} L(x(1))
$$
其中$x(1)$受约束于ODE系统：
$$
\begin{cases}
\dot{x}(t) = u_t(x(t)) \\
x(0) = x_0
\end{cases}
$$
这里$x_0$是唯一被优化的变量，$L$是所需的成本函数。优化通过计算损失对优化变量$x_0$的梯度来实现。

**扩展形式**：可以加入正则化项$R$来扩展目标函数：
$$
\tilde{L}(x) = L(x) + R(x_0, u)
$$
这种表述的通用性使其适用于各种控制生成问题，下面将详细讨论几种具体实例。

### 成本函数示例

#### 反向采样（Reversed Sampling）

考虑最简单的情况$L(x) = \|x - y\|^2$。此时，方程(3)的解将是满足$x(1) = y$的$x_0$，即ODE轨迹在$t=1$时到达$y$。

**关键理论洞察**：由于方程(2)在$u_t(x)$的某些温和假设下定义了一个微分同胚$\mathbb{R}^d \rightarrow \mathbb{R}^d$，对于任意$y \in \mathbb{R}^d$，存在唯一的解$x_0 \in \mathbb{R}^d$满足方程(3)。

#### 逆问题（Inverse Problems）

在逆问题中，我们有已知的退化函数$H: \mathbb{R}^d \rightarrow \mathbb{R}^n$和一个退化样本：
$$
y = H(x^*) + \epsilon
$$
其中$x^*$是未知真实信号，$\epsilon \sim \mathcal{N}(\epsilon)$是可选的加性噪声。目标是恢复产生$y$的$x$，成本函数通常为：
$$
L(x) = \|H(x) - y\|^2
$$
其中范数可以是任意$L_p$范数，甚至是比较$H(x)$和$y$的一般损失$\ell(H(x), y)$。

**具体应用实例**：
- **图像修复**：$H$选择为从$d$个总像素中子采样$n < d$个已知像素
- **图像去模糊**：$H: \mathbb{R}^d \rightarrow \mathbb{R}^d$是模糊函数（如与模糊核的卷积）
- **超分辨率**：$H: \mathbb{R}^d \rightarrow \mathbb{R}^{d/k}$将维度降低$k$倍

#### 条件采样（Conditional Sampling）

条件采样的目标是引导采样过程满足某些条件$y$。例如，如果$F: \mathbb{R}^d \rightarrow \mathbb{R}$是某个函数，我们希望生成属于特定水平集$c \in \mathbb{R}$的样本，可以使用损失：
$$
L(x) = (F(x) - c)^2
$$

### 初始化策略

$x_0$的初始化对优化收敛有重大影响。论文讨论了两种主要初始化策略：

#### 标准初始化

自然的选择是从源分布$p_0(x_0)$中采样$x_0$。

#### 混合初始化

当观测信号$y$对所需$x$提供大量结构信息时（如图像的线性逆问题），可以使用混合初始化加速收敛：
$$
x_0 = \sqrt{\alpha} \cdot y(0) + \sqrt{1 - \alpha} \cdot z
\\[10pt]
z \sim p_0(x_0),\quad y(0) = y + \int_0^1 u(t, y(t))dt
$$
**关键推导**：
- $y(0)$是从$y$反向解ODE得到的源点
- $\alpha$控制混合比例，实验中通常取0.1-0.25
- 这种初始化使优化起点更接近真实解，显著提高收敛速度

### 正则化策略

#### 目标$x(1)$的正则化

将负对数似然(NLL)作为正则化项：
$$
R = -\log p_1(x(1))
$$
这可以通过扩展ODE系统实现：
$$
\begin{cases}
\dot{x}(t) = u_t(x(t)), & x(0) = x_0 \\
\dot{z}(t) = -\text{div}\, u_t(x(t)), & z(0) = \log p_0(x_0)
\end{cases}
$$
解这个ODE系统可得$z(1) = \log p_1(x(1))$

#### 源点$x(0) = x_0$的正则化

对于标准噪声$p_0(x_0) = \mathcal{N}(x_0|0, I)$，NLL正则化为：
$$
R = c + \frac{1}{2}\|x_0\|^2
$$
但这种方法会将$x_0$吸引向均值为零的点，远离大部分概率质量（集中在$\|x_0\| \approx \sqrt{d}$处）。

**改进方法**：使用$\chi_d$分布，定义为随机变量$r = \|x_0\|$的概率分布，其中$x_0 \sim \mathcal{N}(x_0|0, I)$：
$$
R = -\log p(r) = c + (d-1)\log\|x_0\| - \frac{\|x_0\|^2}{2}
$$
**关键推导**：

- $\chi_d$分布的PDF为$p(r) \propto r^{d-1}e^{-r^2/2}$
- 取负对数得到上述正则化项
- 这确保$x_0$保持在$p_0$大部分质量集中的区域

#### 隐式正则化（核心贡献）

论文的核心观点是：对于训练到零损失的标准扩散/流模型，关于$x_0$优化$L(x(1))$会通过将梯度$\nabla_{x(1)}L(x(1))$与局部数据协方差矩阵投影来遵循数据分布$p_1(x_1)$。

**直观解释**：

- 沿着梯度$\nabla_{x(1)}L(x(1))$移动通常会远离数据分布
- 但关于$x(0)$的微分将此梯度投影到高方差数据方向上，从而保持接近数据分布

**关键理论洞察**：
- 通过源点$x_0$优化相当于在数据的主要方差方向上更新样本
- 这种隐式正则化使优化过程自然保持在数据流形上
- 无需显式定义正则化项即可获得高质量结果

### 关键算法流程

D-Flow的步骤：
1. 初始化$x_0^{(0)} = x_0$
2. 对$i = 1, \dots, N$：
   - $x^{(i)}(1) \leftarrow \text{solve}(x_0^{(i)}, u_t)$（解ODE）
   - $x_0^{(i+1)} \leftarrow \text{optimize step}(x_0^{(i)}, \nabla_{x_0}L(x^{(i)}(1)))$（优化步骤）
3. 返回：$x^{(N)}(1)$

## 理论框架核心思想

D-Flow的理论核心是证明：**当对使用高斯概率路径(AGPP)训练的流模型进行源点优化时，梯度$\nabla_{x_{0}}{L(x(1))}$会将$\nabla_{x_{1}}{L(x(1))}$投影到数据的主要变化方向上**。这解释了为什么D-Flow能隐式地将数据先验注入优化过程，使生成样本保持在数据流形上。这些看似复杂的数学公式推导其实是为了证明一个**简单但强大的直觉**：**当你通过生成过程对源点进行微分时，系统会自动将你的优化方向"投影"到数据的自然形状上**。

### Affine Gaussian Probability Paths (AGPP)

#### AGPP定义

AGPP是从噪声分布$p_0$到数据分布$p_1$的概率路径，定义为：

$$
p_t(x) = \int p_t(x|x_1)p_1(x_1)dx_1
$$
其中条件概率为高斯核：
$$
p_t(x|x_1) = \mathcal{N}(x|\alpha_t x_1, \sigma_t^2 I)
$$
$\alpha_t, \sigma_t:[0,1]\rightarrow[0,1]$是调度器(scheduler)，满足：
$$
\alpha_0 = 0, \sigma_1 \approx 0
\\[10pt]
\alpha_1 = 1 = \sigma_0
$$
这保证了$p_t$在$t=0$和$t=1$时分别精确(或近似)地插值源分布和目标分布。

#### 速度场表示

AGPP对应的速度场为：
$$
u_t(x) = \int [a_t x + b_t x_1]p_t(x_1|x)dx_1
\\[10pt]
a_t = \frac{\dot{\sigma}_t}{\sigma_t}, \quad b_t = \dot{\alpha}_t - \alpha_t \frac{\dot{\sigma}_t}{\sigma_t}
$$
利用贝叶斯定理：
$$
p_t(x_1|x) = \frac{p_t(x|x_1)p_1(x_1)}{p_t(x)}
$$
速度场可以简化为：
$$
u_t(x) = a_t x + b_t \hat{x}_{1|t}(x)
$$
其中$\hat{x}_{1|t}(x) = \int x_1 p_t(x_1|x)dx_1$是最优去噪器函数。

#### 去噪器的梯度性质

**Proposition 4.1**：对于AGPP，去噪器$\hat{x}_{1|t}(x)$关于$x$的梯度与随机变量$p_t(x_1|x)$的方差成正比：
$$
D_x \hat{x}_{1|t}(x) = \frac{\alpha_t}{\sigma_t^2} \text{Var}_{1|t}(x)
$$
其中：
$$
\text{Var}_{1|t}(x) = \mathbb{E}_{p_t(x_1|x)}[(x_1 - \hat{x}_{1|t}(x))(x_1 - \hat{x}_{1|t}(x))^T]
$$
**证明过程（Appendix A.1）**：

由AGPP定义：
$$
p_t(x|x_1) = \mathcal{N}(x|\alpha_t x_1, \sigma_t^2 I)
$$
计算梯度：
$$
\nabla_x p_t(x|x_1) = \frac{\alpha_t x_1 - x}{\sigma_t^2} p_t(x|x_1)
$$
计算边际概率的梯度：
$$
\nabla_x p_t(x) = \int \frac{\alpha_t x_1 - x}{\sigma_t^2} p_t(x|x_1)q(x_1)dx_1
$$
由贝叶斯定理：
$$
\nabla_x p_t(x_1|x) = p_t(x_1|x)\frac{\alpha_t}{\sigma_t^2}(x_1 - \hat{x}_{1|t}(x))
$$
代入去噪器的微分：
$$
\begin{aligned}
D_x \hat{x}_{1|t}(x) &= \int \frac{\alpha_t}{\sigma_t^2} p_t(x_1|x)x_1(x_1 - \hat{x}_{1|t}(x))^T dx_1
\\[10pt]
&= \int \frac{\alpha_t}{\sigma_t^2} p_t(x_1|x)(x_1 - \hat{x}_{1|t}(x))(x_1 - \hat{x}_{1|t}(x))^T dx_1
\\[10pt]
&= \frac{\alpha_t}{\sigma_t^2} \text{Var}_{1|t}(x)
\end{aligned}
$$

### 通过求解器的微分分析

#### 连续时间分析

**Theorem 4.2**：对于AGPP速度场$u_t$和由方程2定义的$x(t)$，$x(1)$关于$x_0$的微分为：
$$
D_{x_0}x(1) = \sigma_1 T \exp\left[\int_0^1 \gamma_t \text{Var}_{1|t}(x(t))dt\right]
$$
其中$T \exp[\cdot]$表示时间排序指数，$\gamma_t = \frac{1}{2} \frac{d}{dt}\text{snr}(t)$。

**证明过程（Appendix A.2）**：

定义伴随变量
$$
p(t) = D_{x(t)}x(1)
$$
伴随动力学由以下ODE定义：
$$
\dot{p}(t) = -D_x u_t(x(t))^T p(t)
\\[10pt]
p(1) = D_{x(1)}x(1) = I
$$
通过链式法则可得
$$
D_{x_0}x(1) = p(0)
$$
计算速度场的微分：
$$
\begin{aligned}
D_x u_t(x) &= a_t I + b_t D_x \hat{x}_{1|t}(x)
\\[10pt]
&= a_t I + b_t \frac{\alpha_t}{\sigma_t^2} \text{Var}_{1|t}(x)
\\[5pt]
&= a_t I + \gamma_t \text{Var}_{1|t}(x)
\end{aligned}
$$
其中：
$$
\gamma_t = b_t \frac{\alpha_t}{\sigma_t^2} = \left(\dot{\alpha}_t - \alpha_t \frac{\dot{\sigma}_t}{\sigma_t}\right) \frac{\alpha_t}{\sigma_t^2} = \frac{1}{2} \frac{d}{dt}\text{snr}(t)
$$
代入伴随动力学：
$$
\dot{p}(t) = A(t)p(t),\quad p(1) = I
$$
其中：
$$
A(t) = -(a_t I + \gamma_t \text{Var}_{1|t}(x))^T
$$
非自治线性ODE的解为：
$$
p(t) = T \exp\left[-\int_t^1 A(s)ds\right]p(1)
$$
其中时间排序指数定义为：
$$
T \exp\left[\int_t^1 A(s)ds\right] = \sum_{n=1}^{\infty} \frac{(-1)^n}{n!} \int_t^1 \cdots \int_t^1 T\{A(s_1)A(s_2)\cdots A(s_n)\}ds_1ds_2\cdots ds_n
$$
$T\{A(s_1)A(s_2)\cdots A(s_n)\}$将矩阵乘积按$s_i$值从右到左递减排序。

由于$I$与所有矩阵可交换，得到简化解：
$$
D_{x_0}x(1) = \sigma_1 T \exp\left[\int_0^1 \gamma_t \text{Var}_{1|t}(x)dt\right]
$$
其中$\exp\left[\int_0^1 a_t dt\right] = \sigma_1$（假设$a_t$可积）。

这意味着当你优化源点x₀（而不是直接修改最终图像）时，系统会**自动保持在数据流形上**，也就是说梯度被投影到数据的主要变化方向上，因此，优化过程自然地遵循数据分布，不会产生不自然的样本

即使$\sigma_1 = 0$，积分也是定义良好的（证明见Appendix A.2的Lemma A.3和Proposition A.4）：
- 证明$D_x u_t(x(t))$在$t \rightarrow 1$时有界
- 证明$D_x u_t(x(t))$在$[0,1]$上可积

### 离散时间分析

对于步长为$h = \frac{1}{N}$的欧拉求解器，$x(1)$关于$x_0$的微分为：

$$
D_{x_0}x(1) = \prod_{m=0}^{N-1} \left[(1 + ha_{mh})I + h\gamma_{mh}\text{Var}_{1|mh}(x_{mh})\right]
$$
注意这是时间排序乘积，$m$从右到左递减。

**推导过程（Appendix A.3）**：

欧拉求解器的中间点：
$$
x_{(m+1)h} = x_{mh} + hu_{mh}(x_{mh})
$$
通过链式法则：
$$
D_{x_0}x_1 = \prod_{m=0}^{N-1} D_{x_{mh}}x_{(m+1)h}
$$
代入速度场微分：
$$
D_{x_{mh}}x_{(m+1)h} = (1 + ha_{mh})I + h\gamma_{mh}\text{Var}_{1|mh}(x_{mh})
$$
因此：
$$
D_{x_0}x(1) = \prod_{m=0}^{N-1} \left[(1 + ha_{mh})I + h\gamma_{mh}\text{Var}_{1|mh}(x_{mh})\right]
$$
对于Cond-OT路径（$\alpha_t = t, \sigma_t = 1-t$）：
$$
D_{x_0}x(1) = \prod_{m=0}^{N-1} \left(\frac{1-(m+1)h}{1-mh}I + \frac{mh^2}{(1-mh)^3}\text{Var}_{1|mh}(x_{mh})\right)
$$

### $x(1)$的动力学分析

考虑优化步骤：
$$
x_0^\tau = x_0 - \tau \nabla_{x_0}L(x(1))
$$
其中梯度通过链式法则计算：
$$
\nabla_{x_0}L(x(1)) = D_{x_0}x(1)^T \nabla_{x(1)}L(x(1))
$$

#### $x(1)$的无穷小变化

x(1)的无穷小变化为：
$$
\begin{aligned}
\delta x(1) &= \frac{d}{d\tau}\bigg|_{\tau=0} \Phi(x_0 - \tau \nabla_{x_0}L(x(1)))
\\[10pt]
&= -[D_{x_0}x(1)D_{x_0}x(1)^T]\nabla_{x(1)}L(x(1))
\end{aligned}
$$
其中$\Phi: \mathbb{R}^d \rightarrow \mathbb{R}^d$是将初始条件$x_0$映射到方程2在$t=1$时的解的映射。

#### 隐式正则化解释

这一结果揭示了D-Flow的隐式正则化机制：

1. $D_{x_0}x(1)$由时间排序指数$T \exp\left[\int_0^1 \gamma_t \text{Var}_{1|t}(x(t))dt\right]$组成
2. $\text{Var}_{1|t}(x(t))$是条件数据协方差矩阵
3. $D_{x_0}x(1)D_{x_0}x(1)^T$将梯度$\nabla_{x(1)}L(x(1))$投影到数据的主要变化方向上
4. 这种投影确保优化过程保持在数据流形附近

直观上，$\text{Var}_{1|t}(x(t))v$将向量$v$投影到数据分布的主轴上。因此，$D_{x_0}x(1)$迭代地应用这种投影，使优化过程自然地遵循数据分布$p_1(x)$。

## AGPP与不同模型的对应关系（Appendix A.4）

### 一般AGPP表示

考虑由传输映射定义的一般仿射条件概率路径：
$$
x_t = \sigma_t x_0 + \alpha_t x_1
$$
其中$x_0 \sim p_0$且$x_1 \sim p_1$。

条件速度场为：
$$
u_t(x|x_1) = \frac{\dot{\sigma}_t}{\sigma_t}(x - \alpha_t x_1) + \dot{\alpha}_t x_1 = \frac{\dot{\sigma}_t}{\sigma_t}x - \left(\frac{\dot{\sigma}_t\alpha_t}{\sigma_t} - \dot{\alpha}_t\right)x_1
\\[10pt]
u_t(x|x_0) = \dot{\sigma}_t x_0 + \frac{\dot{\alpha}_t}{\alpha_t}(x - \sigma_t x_0) = \frac{\dot{\alpha}_t}{\alpha_t}x - \left(\frac{\dot{\alpha}_t\sigma_t}{\alpha_t} - \dot{\sigma}_t\right)x_0
$$
边缘速度场可表示为：
$$
u_t(x) = \frac{\dot{\sigma}_t}{\sigma_t}x - \left(\frac{\dot{\sigma}_t\alpha_t}{\sigma_t} - \dot{\alpha}_t\right)\hat{x}_{1|t}(x)
$$
其中$\hat{x}_{1|t}(x)$是最优去噪器函数。

### 4.2 Cond-OT路径特例

当$\alpha_t = t, \sigma_t = 1-t$时（Cond-OT路径）：
$$
u_t(x) = \frac{\hat{x}_{1|t}(x) - x}{1 - t}
$$
或者用最优噪声预测器$\epsilon_t(x)$表示：
$$
u_t(x) = \frac{x - \epsilon_t(x)}{t}
$$

### 理论意义总结

1. **隐式正则化机制**：Theorem 4.2表明，$D_{x_0}x(1)$本质上是数据协方差矩阵$\text{Var}_{1|t}(x(t))$的时间排序指数，这解释了为什么源点优化会产生隐式正则化。

2. **离散时间有效性**：Appendix A.3证明，即使使用少量求解器步骤，$D_{x_0}x(1)$仍包含协方差矩阵的幂，这解释了为什么D-Flow在实践中有效。

3. **投影解释**：$D_{x_0}x(1)D_{x_0}x(1)^T$将梯度$\nabla_{x(1)}L(x(1))$投影到数据分布的主要方向上，确保优化过程保持在数据流形附近。

4. **通用性**：该理论适用于所有使用AGPP训练的流模型，包括扩散模型和流匹配模型。

这些理论结果完美解释了D-Flow的实验观察：在优化过程中，中间样本$x^{(0)}(1), x^{(2)}(1), x^{(4)}(1), \dots$保持接近数据分布，并通过不同的数据子类型。
